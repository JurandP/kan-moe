# Kolmogorov-Arnold Networks within the Mixture of Experts
A student research project evaluating the efficiency of Kolmogorov-Arnold Networks within the Mixture of Experts transformer architecture.

# Abstract
Kolmogorov-Arnold Networks (KAN) present a promising alternative to standard Multi-Layer Perceptrons (MLP), offering advantages in scalability and interpretability. Meanwhile, the Mixture of Experts (MoE) Transformer represents a state-of-the-art architecture in Large Language Models (LLMs), traditionally utilizing MLP-based networks as experts. In this study, we investigated the impact of replacing MLP experts with KANs. Our findings suggest that this modification generally results in inferior performance, indicating that KAN may not be beneficial in this context.

# Full Report
The complete research report is available in [report.pdf](report.pdf).